{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excited-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinate-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "subject  S1\n",
      "subject  S5\n",
      "subject  S6\n",
      "subject  S7\n",
      "subject  S8\n",
      "subject  S9\n",
      "subject  S11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) \n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import cv2\n",
    "from termcolor import colored\n",
    "import utils \n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import image_components\n",
    "import pose_components\n",
    "import matplotlib.pyplot as plt\n",
    "import dataloader\n",
    "from test_data_loader import get_h36_test, data_loader_h36\n",
    "from hyperparams import Hyperparameters\n",
    "import multiprocessing as mp \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "universal-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Hyperparameters ()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = H.cuda_device_id\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-southwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of H3.6M test set: 2181\n"
     ]
    }
   ],
   "source": [
    "h36_test_data = get_h36_test()\n",
    "h36_test_dataset = tf.data.Dataset.from_tensor_slices((h36_test_data))\n",
    "dataset_human_test = h36_test_dataset.map(lambda z: tf.py_func(data_loader_h36, [z], [tf.float32, \\\n",
    "        tf.float32]), 32)\n",
    "dataset_human_test = dataset_human_test.batch(H.batch_size)\n",
    "dataset_human_test = dataset_human_test.prefetch(2)\n",
    "test_iterator = dataset_human_test.make_initializable_iterator()\n",
    "test_img, test_pose = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "veterinary-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ph = tf.placeholder(tf.float32 , shape = [None, H.image_size,H.image_size ,3] , name = 'image_ph')\n",
    "pose_ph = tf.placeholder(tf.float32 , shape = [None,17,3] , name = 'pose_gt_ph')\n",
    "\n",
    "def get_image_to_latent(input_image): # input_image shape:[None,224,224,3]\n",
    "    resnet_out = image_components.create_network(input_image,'')\n",
    "    resnet_params = image_components.get_network_params('')\n",
    "    fc_embedding = image_components.embedding_branch(resnet_out)\n",
    "    fc_embedding_params = image_components.get_network_params('embedding')\n",
    "    outputs = {\n",
    "        'resnet_params' :resnet_params,\n",
    "        'im_embed' :fc_embedding,\n",
    "        'fc_embedding_params':fc_embedding_params\n",
    "    }\n",
    "    return outputs\n",
    "    \n",
    "cnn_outputs = get_image_to_latent(image_ph)\n",
    "embed=cnn_outputs['im_embed']\n",
    "pose_pred = pose_components.apply_pose_decoder(embed)\n",
    "\n",
    "loss = tf.reduce_mean(tf.abs(pose_pred - pose_ph))\n",
    "loss_summary = tf.summary.scalar(\"pose_pred_loss\",loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "\n",
    "resnet_params = cnn_outputs['resnet_params']\n",
    "fc_embedding_params = cnn_outputs['fc_embedding_params']\n",
    "pose_decoded_params = pose_components.get_network_params('Decoder_net')\n",
    "\n",
    "opt = tf.train.AdamOptimizer(H.learning_rate).minimize(loss = loss,\n",
    "                                                       var_list = resnet_params + fc_embedding_params)\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "resnet_branch_weights = tf.train.Saver(resnet_params,max_to_keep=5)\n",
    "embedding_branch_weights = tf.train.Saver(fc_embedding_params,max_to_keep=5)\n",
    "decoder_branch_weights = tf.train.Saver(pose_decoded_params,max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worst-specific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrestored resnet branch weights\u001b[0m\n",
      "\u001b[32mRestored Embedding Branch weights\u001b[0m\n",
      "\u001b[34mRestored Decoder weights\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.train.Saver(resnet_params).restore(sess,tf.train.latest_checkpoint('./weights/resnet/'))\n",
    "print (colored(\"restored resnet branch weights\",\"yellow\"))\n",
    "\n",
    "tf.train.Saver(fc_embedding_params).restore(sess,tf.train.latest_checkpoint('./weights/embedding_branch/'))\n",
    "print (colored(\"Restored Embedding Branch weights\",\"green\"))\n",
    "\n",
    "tf.train.Saver(pose_decoded_params).restore(sess,'../pretrained_weights/pose_encoder_decoder/decoder_iter-1475001')\n",
    "print (colored(\"Restored Decoder weights\",\"blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elementary-stock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss :  241.15142313639322\n",
      "241.15142313639322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_list = []\n",
    "sess.run(test_iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        images, pose_3d =  sess.run([test_img, test_pose])\n",
    "        val_loss,pred_3d = sess.run([loss,pose_pred], feed_dict = {image_ph : images , pose_ph : pose_3d})\n",
    "        loss_list.append(val_loss) \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Val loss : \",sum(loss_list)/len(loss_list))\n",
    "        break\n",
    "print (sum(loss_list)/len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-reader",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
