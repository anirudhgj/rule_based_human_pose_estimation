{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-hurricane",
   "metadata": {},
   "source": [
    "# Adapting from Source Domain to Target Domain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-experiment",
   "metadata": {},
   "source": [
    "## Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vietnamese-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:tensorflow:From /data/vcl/anirudh_rule_based/codes_2021/neurips_camera/SA3DHumanPose/target_adaptation/motion_components.py:14: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.image as ming\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from termcolor import colored\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import re\n",
    "\n",
    "import motion_components\n",
    "import cnn_model\n",
    "from data_loader import datasets, data_loaders\n",
    "from hyperparams import Hyperparameters\n",
    "H = Hyperparameters ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-harvard",
   "metadata": {},
   "source": [
    "## Session Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = H.cuda_device_id\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-wilson",
   "metadata": {},
   "source": [
    "## Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incoming-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [tf.data.Dataset.from_tensor_slices((x)) for x in datasets] \n",
    "for idx, data_loader in enumerate(data_loaders):\n",
    "    datasets[idx] = datasets[idx].map(lambda z: tf.py_func(data_loader, [z], [tf.float32, \\\n",
    "            tf.float32]), 32)\n",
    "\n",
    "dset_names = [\"l_lcr\", \"l_hcr\", \"l_1_z\", \"l_1_v\", \"l_2_v\"]\n",
    "datasets = [dataset.shuffle(buffer_size = 10) for dataset in datasets]\n",
    "datasets = [dataset.batch(H.batch_size) for dataset in datasets]\n",
    "datasets = [dataset.prefetch(10) for dataset in datasets]\n",
    "dataset_iterators = [dataset.make_initializable_iterator() for dataset in datasets]\n",
    "string_handles = [sess.run(i.string_handle()) for i in dataset_iterators]\n",
    "handle_pl = tf.placeholder(tf.string, shape=[])\n",
    "base_iterator = tf.data.Iterator.from_string_handle(handle_pl, datasets[0].output_types, datasets[0].output_shapes)\n",
    "\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.makedirs('./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-norwegian",
   "metadata": {},
   "source": [
    "## Util Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decimal-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_flip_tr(pose_embed,name='FlipRule'): # pose_embed shape : [None,32]\n",
    "    mapped = motion_components.apply_rule_net(pose_embed,name, state_size = 32)\n",
    "    return mapped \n",
    "    \n",
    "def flip_backward_tr(motion_embed,name='flip_backward'): # motion_embed shape : [batch_size,seq_length,128]\n",
    "    mapped = motion_components.apply_motion_rule_net(motion_embed,name)\n",
    "    rule_state = mapped['mapped_state'] \n",
    "    return rule_state\n",
    "\n",
    "def slow_backward_tr(motion_embed,name='SlowBackward'): # motion_embed shape : [batch_size,seq_length,128]\n",
    "    mapped = motion_components.apply_motion_rule_net(motion_embed,name)\n",
    "    rule_state = mapped['mapped_state'] \n",
    "    return rule_state\n",
    "\n",
    "def motion_encoder(encoder_input,name='motion_encoder'):\n",
    "    encoder_lstm_out=motion_components.apply_encoder(encoder_input,name)\n",
    "    z_state = encoder_lstm_out['z_state']\n",
    "    return z_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-wisdom",
   "metadata": {},
   "source": [
    "## Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loaded-fraud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/vcl/anirudh_rule_based/codes_2021/neurips_camera/SA3DHumanPose/target_adaptation/cnn_model.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /data/vcl/anirudh_rule_based/codes_2021/neurips_camera/SA3DHumanPose/target_adaptation/cnn_model.py:9: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/vcl/anirudh_rule_based/codes_2021/neurips_camera/SA3DHumanPose/target_adaptation/cnn_model.py:9: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_seq, positive_seq = base_iterator.get_next()\n",
    "image_seq = tf.reshape(image_seq,[H.batch_size * H.seq_length, 224, 224, 3])\n",
    "positive_seq = tf.reshape(positive_seq,[H.batch_size * H.seq_length, 224, 224, 3])\n",
    "\n",
    "image_features = cnn_model.create_network(image_seq,'')\n",
    "resnet_params = cnn_model.get_network_params('')\n",
    "train_params = [x for x in resnet_params if \"res3\" in x.op.name]\n",
    "image_embedding, image_rep = cnn_model.embedding_branch(image_features)\n",
    "\n",
    "positive_features = cnn_model.create_network(positive_seq, '')\n",
    "positive_embedding, positive_rep = cnn_model.embedding_branch(positive_features)\n",
    "\n",
    "image_cr_embedding = cnn_model.mlp_head(image_rep, \"pose_mlp_head\")\n",
    "pos_cr_embedding = cnn_model.mlp_head(positive_rep, \"pose_mlp_head\")\n",
    "\n",
    "train_params_pose = cnn_model.get_network_params(\"pose_mlp_head\")\n",
    "\n",
    "image_cr_seq = tf.reshape(image_cr_embedding, [H.batch_size, H.seq_length, 128])\n",
    "pos_cr_seq = tf.reshape(pos_cr_embedding, [H.batch_size, H.seq_length, 128])\n",
    "\n",
    "image_seq_embedding = tf.reshape(image_embedding, [H.batch_size, H.seq_length, 32])\n",
    "positive_seq_embedding = tf.reshape(positive_embedding, [H.batch_size, H.seq_length, 32])\n",
    "positive_seq_embedding = tf.stop_gradient(positive_seq_embedding)\n",
    "\n",
    "\n",
    "motion_embedding = motion_encoder(image_seq_embedding)\n",
    "pos_motion_embedding = motion_encoder(positive_seq_embedding)\n",
    "\n",
    "motion_cr_embedding = cnn_model.mlp_head(motion_embedding, \"motion_mlp_head\")\n",
    "positive_cr_embedding = cnn_model.mlp_head(pos_motion_embedding, \"motion_mlp_head\")\n",
    "\n",
    "train_params_motion = cnn_model.get_network_params(\"motion_mlp_head\")\n",
    "\n",
    "l1_loss = lambda gt, pred : tf.reduce_mean(tf.abs(gt-pred))\n",
    "\n",
    "#############\n",
    "# Pose flip\n",
    "#############\n",
    "\n",
    "flipped_embedding = pose_flip_tr(image_embedding)\n",
    "loss1 = l1_loss(flipped_embedding, positive_embedding)\n",
    "opt1 = tf.train.AdamOptimizer(H.learning_rate).minimize(loss = loss1,var_list = train_params)\n",
    "#############\n",
    "# Flip B/W\n",
    "#############\n",
    "\n",
    "flip_bw_embedding = flip_backward_tr(motion_embedding)\n",
    "loss2 = l1_loss(flip_bw_embedding, pos_motion_embedding)\n",
    "opt2 = tf.train.AdamOptimizer(H.learning_rate).minimize(loss = loss2,var_list = train_params)\n",
    "#############\n",
    "# Slow B/W\n",
    "#############\n",
    "slow_bw_embedding = slow_backward_tr(motion_embedding)\n",
    "loss3 = l1_loss(slow_bw_embedding, pos_motion_embedding)\n",
    "opt3 = tf.train.AdamOptimizer(H.learning_rate).minimize(loss = loss3,var_list = train_params)\n",
    "\n",
    "\n",
    "############\n",
    "# Higher order CR loss\n",
    "############\n",
    "T = 0.07\n",
    "motion_embedding = tf.math.l2_normalize(motion_cr_embedding, axis = 1)\n",
    "pos_motion_embedding = tf.math.l2_normalize(positive_cr_embedding, axis = 1)\n",
    "\n",
    "\n",
    "pos = tf.exp(tf.reduce_sum(motion_embedding[0, :] * pos_motion_embedding[0, :])/T)\n",
    "tot = tf.reduce_sum(tf.exp(tf.reduce_sum(motion_embedding[0,:] * motion_embedding[1:, :], 1)/T))\n",
    "tot = tot + pos\n",
    "loss4 = -tf.log(pos/tot)\n",
    "\n",
    "opt4 = tf.train.AdamOptimizer(H.learning_rate*2).minimize(loss = loss4,var_list = train_params+train_params_motion)\n",
    "\n",
    "#############\n",
    "# Lower order CR loss\n",
    "#############\n",
    "\n",
    "#sample every 4th frame of the sequence, giving us 8 frames sampled from each seq.\n",
    "image_seq_embedding = tf.math.l2_normalize(image_cr_seq[:, 0, :], axis = 1)\n",
    "positive_seq_embedding = tf.math.l2_normalize(pos_cr_seq[:, 0, :], axis = 1)\n",
    "\n",
    "image_seq_embedding = tf.reshape(image_seq_embedding, [H.batch_size, 128])\n",
    "positive_seq_embedding = tf.reshape(positive_seq_embedding, [H.batch_size, 128])\n",
    "\n",
    "pos = tf.exp(tf.reduce_sum(image_seq_embedding[0,:] * positive_seq_embedding[0,:])/T)\n",
    "tot = tf.reduce_sum(tf.exp(tf.reduce_sum(image_seq_embedding[0,:] * image_seq_embedding[1:, :], 1)/T))\n",
    "tot = tot + pos\n",
    "loss5 = -tf.log(pos/tot)\n",
    "\n",
    "\n",
    "opt5 = tf.train.AdamOptimizer(H.learning_rate*2).minimize(loss = loss5,var_list = train_params+train_params_pose)\n",
    "\n",
    "\n",
    "#############\n",
    "# add all train ops to a list\n",
    "#############\n",
    "opts = [opt5, opt4, opt1, opt2, opt3]\n",
    "train_ops = opts #[]\n",
    "losses = [loss5, loss4, loss1, loss2, loss3]\n",
    "\n",
    "\n",
    "\n",
    "pose_flip_tr_params = cnn_model.get_network_params(\"FlipRule\")\n",
    "flip_backward_tr_params = cnn_model.get_network_params(\"flip_backward\")\n",
    "slow_backward_tr_params = cnn_model.get_network_params(\"SlowBackward\")\n",
    "motion_encoder_params = cnn_model.get_network_params(\"motion_encoder\")\n",
    "embedding_branch_params = cnn_model.get_network_params(\"embedding\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-chapter",
   "metadata": {},
   "source": [
    "## Saving Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "treated-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./weights_intermediate/resnet/cnn_stickman_training7200\n",
      "\u001b[33mrestored resnet branch weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../pretrained_weights/pose_rule_flip/relation_tr_47999\n",
      "INFO:tensorflow:Restoring parameters from ../pretrained_weights/motion_rule_flip_backward/motion_net_expt_seq30_HuMaMpi56900\n",
      "INFO:tensorflow:Restoring parameters from ../pretrained_weights/motion_rule_slow_backward/motion_rule_slow_backward9000\n",
      "\u001b[33mrestored transformer weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../pretrained_weights/motion_encoder_decoder/lstm_encoder/motion_net_expt_seq30_HuMaMpi91500\n",
      "\u001b[33mrestored motion encoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ./weights_intermediate/embedding_branch/cnn_stickman_training7200\n",
      "\u001b[33mrestored embedding branch weights\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Saving weights\n",
    "'''\n",
    "resnet_branch_weights = tf.train.Saver(resnet_params,max_to_keep=5)\n",
    "train_writer = tf.summary.FileWriter(H.logdir_path_train,sess.graph)\n",
    "\n",
    "'''\n",
    "Loading weights\n",
    "'''\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.train.Saver(resnet_params).restore(sess,tf.train.latest_checkpoint(H.store_resnet_weights_path))\n",
    "print(colored(\"restored resnet branch weights\",\"yellow\"))\n",
    "\n",
    "tf.train.Saver(pose_flip_tr_params).restore(sess, tf.train.latest_checkpoint(H.flip_tr_path))\n",
    "tf.train.Saver(flip_backward_tr_params).restore(sess, tf.train.latest_checkpoint(H.fb_tr_path))\n",
    "tf.train.Saver(slow_backward_tr_params).restore(sess, tf.train.latest_checkpoint(H.sb_tr_path))\n",
    "print(colored(\"restored transformer weights\",\"yellow\"))\n",
    "\n",
    "tf.train.Saver(motion_encoder_params).restore(sess, H.motion_encoder_path)\n",
    "print(colored(\"restored motion encoder weights\",\"yellow\"))\n",
    "\n",
    "tf.train.Saver(embedding_branch_params).restore(sess, tf.train.latest_checkpoint(H.store_embedding_branch_weights_path))\n",
    "print(colored(\"restored embedding branch weights\",\"yellow\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-unemployment",
   "metadata": {},
   "source": [
    "## Saving logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "limited-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary writer\n",
    "'''\n",
    "def get_summary_str(val, tag):\n",
    "\n",
    "    summary_str = tf.Summary()\n",
    "    summary_str.value.add(tag = tag, simple_value = val)\n",
    "\n",
    "    return summary_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-identification",
   "metadata": {},
   "source": [
    "## Adaptation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "measured-adrian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training loop!\n",
      "Iteration 0: loss l_lcr value : 2.70884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration = 0\n",
    "\n",
    "for iterator in dataset_iterators:\n",
    "    sess.run(iterator.initializer)\n",
    "\n",
    "print(\"starting training loop!\")\n",
    "while True:\n",
    "    dset = iteration%5\n",
    "    try:\n",
    "        lval, _ = sess.run([losses[dset], train_ops[dset]], feed_dict = \\\n",
    "                {handle_pl : string_handles[dset]})\n",
    "\n",
    "        print(\"Iteration %d: loss %s value : %.5f\"%(iteration, dset_names[dset], lval))\n",
    "        summary_str = get_summary_str(lval, \"losses/%s\"%dset_names[dset])\n",
    "        train_writer.add_summary(summary_str, iteration)\n",
    "        train_writer.flush() # write to disk now\n",
    "        iteration += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(dataset_iterators[dset].initializer)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instrumental-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio \n",
    "k = sio.loadmat('./sample_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tropical-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'frames', 'slow_frames'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lovely-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k['slow_frames'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-father",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit5ad04f25011a4088812e4699189446c2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
